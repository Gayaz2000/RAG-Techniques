{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955e42a6",
   "metadata": {},
   "source": [
    "# ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb45459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7534cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\", max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed5728a",
   "metadata": {},
   "source": [
    "#Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aef03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "        Replace all the tab ('\\t') keys with white space in the page content of list of documents.\n",
    "\n",
    "        Args:\n",
    "            list_of_documents: A list of document obj, each with 'page_content' attribute.\n",
    "        Return:\n",
    "            The modified list of documents with tab characters replaced by white spaces\n",
    "    \"\"\"\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', \" \")\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9122e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#from helper_functions import Helpers\n",
    "\n",
    "class Data_Ingestion_Pipe:\n",
    "    \"\"\"\n",
    "    A pipeline that showcases the ingestion of documet data into vectorstore\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str = r\"D:\\My Files\\RAG-Techniques\\RAG.pdf\"):\n",
    "        self.file_path = file_path\n",
    "        #self.helper_func = Helpers()\n",
    "        #self.embed_provider = Embedding_Provider()\n",
    "\n",
    "    \n",
    "    async def encode_pdf(self, chunk_size: int =1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Set of setps to stores the pdf documents in vectorestore in the form of embeddings\n",
    "        Args:\n",
    "            file_path: denotes the location of the file\n",
    "            chunk_size : denote the size of each chunk the document to be split into\n",
    "            chunk_overlap: connecting words in each chunk\n",
    "\n",
    "        Return:\n",
    "            A FAISS vector store containing the encoded book content.\n",
    "        \"\"\"\n",
    "        #loads the pdf file\n",
    "        try:\n",
    "            loader = PyPDFLoader(self.file_path)\n",
    "            docs = await loader.aload()\n",
    "        except FileNotFoundError as e:\n",
    "            raise f\"Error occured: {e}\"\n",
    "        # split the doc file into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size, chunk_overlap = chunk_overlap\n",
    "        )\n",
    "        doc_chunks = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "        cleaned_texts = replace_t_with_space(doc_chunks)\n",
    "        #embeddings\n",
    "        embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        #vector db\n",
    "        faiss_vstore = await FAISS.afrom_documents(cleaned_texts, embedding=embedding)\n",
    "        return faiss_vstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12074cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Files\\RAG-Techniques\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "vectorstore = asyncio.run(Data_Ingestion_Pipe().encode_pdf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1107e8",
   "metadata": {},
   "source": [
    "#Reranking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d93e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Ranking(BaseModel):\n",
    "    relevance_score: float = Field(default=0.0, description=\"Relevance score of the document to the query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a2f5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "async def rerank_documents(query: str, documents: List[Document], top_k: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Rerank the documents based on their relevance to the query using a language model.\n",
    "    Args:\n",
    "        query (str): The query to rerank the documents against.\n",
    "        documents (List[Document]): The list of documents to rerank.\n",
    "        top_k (int): The number of top documents to return.\n",
    "    Returns:\n",
    "        List[Document]: The reranked list of documents.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\", \"documents\"],\n",
    "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "        Query: {query}\n",
    "        Document: {doc}\n",
    "        Relevance Score:\"\"\"\n",
    "    )\n",
    "    llm_chain = prompt_template | llm.with_structured_output(Ranking)\n",
    "\n",
    "    scored_documents = []\n",
    "    for doc in documents:\n",
    "        input_data = {\"query\": query,\"doc\": doc.page_content}\n",
    "        response = await llm_chain.ainvoke(input_data)\n",
    "        score = response.relevance_score\n",
    "        try:\n",
    "            score = float(score)\n",
    "        except ValueError:\n",
    "            score = 0.0\n",
    "        scored_documents.append((doc, score))\n",
    "    reranked_= sorted(scored_documents, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked_[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa1e40",
   "metadata": {},
   "source": [
    "#TestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d170b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the purpose of the RAG technique in AI?\"\n",
    "initial_docs = vectorstore.similarity_search(query, k=5)\n",
    "reranked_docs = rerank_documents(query, initial_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bf57fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top initial documents:\n",
      "\n",
      "Document 1:\n",
      "ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\n",
      "runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\n",
      "...\n",
      "\n",
      "Document 2:\n",
      "Broader Impact\n",
      "This work offers several positive societal beneﬁts over previous work: the fact that it is more\n",
      "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinat...\n",
      "\n",
      "Document 3:\n",
      "2 Methods\n",
      "We explore RAG models, which use the input sequencex to retrieve text documents z and use them\n",
      "as additional context when generating the target sequence y . As shown in Figure 1, our models\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:3]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0aa4560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the purpose of the RAG technique in AI?\n",
      "\n",
      "Top reranked documents:\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "rerank_response = asyncio.run(reranked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e390cea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\n",
      "at https://huggingface.co/rag/\n",
      "2https://github.com/pytorch/fairseq\n",
      "3https://github.com/huggingface/transformers\n",
      "1...\n",
      "\n",
      "Document 2:\n",
      "2 Methods\n",
      "We explore RAG models, which use the input sequencex to retrieve text documents z and use them\n",
      "as additional context when generating the target sequence y . As shown in Figure 1, our models\n",
      "...\n",
      "\n",
      "Document 3:\n",
      "ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and\n",
      "runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(rerank_response):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833709a5",
   "metadata": {},
   "source": [
    "#Custom Retriever based on ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c45fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1220\\2608581287.py:4: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class CustomRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing_extensions import Any\n",
    "\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "\n",
    "    vectorstore: Any = Field(description=\"vectorstore for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=10)\n",
    "        response = asyncio.run(rerank_documents(query, initial_docs))\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b0de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3420556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever= custom_retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d076d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the purpose of the RAG technique in AI?\n",
      "Answer: According to the provided context, the purpose of the RAG (Regressive Attention Generative) technique is to retrieve text documents (z) based on the input sequence (x) and use them as additional context when generating the target sequence (y). The technique combines a retriever (DPR) and a generator to model the probability distribution over the target sequence. The retriever retrieves relevant documents based on the input sequence, and the generator uses these documents to generate the target sequence.\n",
      "\n",
      "Relevant source documents:\n",
      "{'query': 'What is the purpose of the RAG technique in AI?', 'result': 'According to the provided context, the purpose of the RAG (Regressive Attention Generative) technique is to retrieve text documents (z) based on the input sequence (x) and use them as additional context when generating the target sequence (y). The technique combines a retriever (DPR) and a generator to model the probability distribution over the target sequence. The retriever retrieves relevant documents based on the input sequence, and the generator uses these documents to generate the target sequence.', 'source_documents': [Document(id='23b339cb-dde4-4ddc-a5ee-5c6a431e41e0', metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\My Files\\\\RAG-Techniques\\\\RAG.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='2 Methods\\nWe explore RAG models, which use the input sequencex to retrieve text documents z and use them\\nas additional context when generating the target sequence y . As shown in Figure 1, our models\\nleverage two components: (i) a retriever p η(z |x ) with parameters η that returns (top-K truncated)\\ndistributions over text passages given a query x and (ii) a generator p θ(y i |x,z,y 1:i −1) parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2'), Document(id='6412dc79-4121-40cf-a726-21e6860e1ca9', metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\My Files\\\\RAG-Techniques\\\\RAG.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nat https://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17'), Document(id='d0e8d082-586b-40ae-98ea-1e8865c45ee8', metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\My Files\\\\RAG-Techniques\\\\RAG.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN∏\\ni\\n∑\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x,z,y 1:i−1)\\nFinally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n(\\nd(z)⊤q(x)\\n)\\nd(z) =BERTd(z), q(x) =BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating\\ntop-k(pη(·|x)), the list of kdocuments zwith highest prior probability pη(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use')]}\n"
     ]
    }
   ],
   "source": [
    "result =  qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943c59f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "2 Methods\n",
      "We explore RAG models, which use the input sequencex to retrieve text documents z and use them\n",
      "as additional context when generating the target sequence y . As shown in Figure 1, our models\n",
      "...\n",
      "\n",
      "Document 2:\n",
      "blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\n",
      "at https://huggingface.co/rag/\n",
      "2https://github.com/pytorch/fairseq\n",
      "3https://github.com/huggingface/transformers\n",
      "1...\n",
      "\n",
      "Document 3:\n",
      "before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\n",
      "pRAG-Token(y|x) ≈\n",
      "N∏\n",
      "i\n",
      "∑\n",
      "z∈top-k(p(·|x))\n",
      "pη(z|x)pθ(yi|x,z,y 1:i−1)\n",
      "Finally, we note that RAG can be...\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a818d",
   "metadata": {},
   "source": [
    "#Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54fc04de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Files\\RAG-Techniques\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1220\\361147625.py:5: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1220\\361147625.py:5: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class CrossEncoderRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Initial retrieval\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # Prepare pairs for cross-encoder\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Sort documents by score\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top reranked documents\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "458a79ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is RAG in AI?\n",
      "Answer: RAG stands for Retrieval-Augmented Generation, which is a type of artificial intelligence (AI) model that combines the strengths of both retrieval-based and generation-based language models.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Broader Impact\n",
      "This work offers several positive societal beneﬁts over previous work: the fact that it is more\n",
      "strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinat...\n",
      "\n",
      "Document 2:\n",
      "to a lesser extent, including that it might be used to generate abuse, faked or misleading content in\n",
      "the news or on social media; to impersonate others; or to automate the production of spam/phishing...\n",
      "\n",
      "Document 3:\n",
      "2 Methods\n",
      "We explore RAG models, which use the input sequencex to retrieve text documents z and use them\n",
      "as additional context when generating the target sequence y . As shown in Figure 1, our models\n",
      "...\n",
      "\n",
      "Document 4:\n",
      "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
      "4.5 Additional Results\n",
      "Generation Diversity Section 4.3 shows that RAG models are more factual and spec...\n",
      "\n",
      "Document 5:\n",
      "before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:\n",
      "pRAG-Token(y|x) ≈\n",
      "N∏\n",
      "i\n",
      "∑\n",
      "z∈top-k(p(·|x))\n",
      "pη(z|x)pθ(yi|x,z,y 1:i−1)\n",
      "Finally, we note that RAG can be...\n"
     ]
    }
   ],
   "source": [
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    cross_encoder=cross_encoder,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain with the cross-encoder retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What is RAG in AI?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4bc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
