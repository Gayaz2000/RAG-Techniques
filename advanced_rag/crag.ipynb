{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2b40f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae8e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_t_with_space(list_of_documents):\n",
    "    \"\"\"\n",
    "        Replace all the tab ('\\t') keys with white space in the page content of list of documents.\n",
    "\n",
    "        Args:\n",
    "            list_of_documents: A list of document obj, each with 'page_content' attribute.\n",
    "        Return:\n",
    "            The modified list of documents with tab characters replaced by white spaces\n",
    "    \"\"\"\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', \" \")\n",
    "    return list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b6071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#from helper_functions import Helpers\n",
    "\n",
    "class Data_Ingestion_Pipe:\n",
    "    \"\"\"\n",
    "    A pipeline that showcases the ingestion of documet data into vectorstore\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str = r\"D:\\My Files\\RAG-Techniques\\RAG.pdf\"):\n",
    "        self.file_path = file_path\n",
    "        #self.helper_func = Helpers()\n",
    "        #self.embed_provider = Embedding_Provider()\n",
    "\n",
    "    \n",
    "    def encode_pdf(self, chunk_size: int =1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Set of setps to stores the pdf documents in vectorestore in the form of embeddings\n",
    "        Args:\n",
    "            file_path: denotes the location of the file\n",
    "            chunk_size : denote the size of each chunk the document to be split into\n",
    "            chunk_overlap: connecting words in each chunk\n",
    "\n",
    "        Return:\n",
    "            A FAISS vector store containing the encoded book content.\n",
    "        \"\"\"\n",
    "        #loads the pdf file\n",
    "        try:\n",
    "            loader = PyPDFLoader(self.file_path)\n",
    "            docs = loader.load()\n",
    "        except FileNotFoundError as e:\n",
    "            raise f\"Error occured: {e}\"\n",
    "        # split the doc file into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size, chunk_overlap = chunk_overlap\n",
    "        )\n",
    "        doc_chunks = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "        cleaned_texts = replace_t_with_space(doc_chunks)\n",
    "        #embeddings\n",
    "        embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        #vector db\n",
    "        faiss_vstore = FAISS.from_documents(cleaned_texts, embedding=embedding)\n",
    "        return faiss_vstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f501b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_retriever():\n",
    "    \"\"\"\n",
    "    retrieves top k similar documents\n",
    "    \"\"\"\n",
    "    obj = Data_Ingestion_Pipe()\n",
    "    vstore = obj.encode_pdf()\n",
    "    retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ae0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "261a4324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "web_search = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e32193",
   "metadata": {},
   "source": [
    "#Retriever Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98c15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RetrieverInputEvaluator(BaseModel):\n",
    "    relevance_score: float = Field(..., description=\"describe the relevance of retrieved document to query. The score should be between 0 and 1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bceead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def retrieveal_evaluator(query: str, document: str)-> float:\n",
    "    \"\"\"\n",
    "    Evaluates the relevance of retrieved documents with the query\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"document\"],\n",
    "        template=\"\"\"\n",
    "        On the scale of 0 to 1 how much relevent is the document to the following query.\n",
    "        \\nQuery: {query}\n",
    "        \\nDocument: {document}\n",
    "        Relevance Score:\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(RetrieverInputEvaluator)\n",
    "    input_variables = {\"query\": query, \"document\": document}\n",
    "    result = chain.invoke(input_variables).relevance_score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecb913",
   "metadata": {},
   "source": [
    "#Knowledge Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f4ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeInputRefinement(BaseModel):\n",
    "    key_points: str = Field(..., description=\"documents to extract key information from\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f197c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List\n",
    "\n",
    "def knowledge_refinement(document: str)->List[str]:\n",
    "    \"\"\"\n",
    "    Takes out key points from the retrieved and combined documents\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"document\"],\n",
    "        template=\"\"\"\n",
    "            Extract the key points from the following documents in bullet points format:\n",
    "            \\nDocuments: {documents}\n",
    "            \\nKey points:\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(KnowledgeInputRefinement)\n",
    "    result = chain.invoke({\"document\": document}).key_points\n",
    "    return [point.strip() for point in result.split('\\n') if point.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d81312",
   "metadata": {},
   "source": [
    "#Query Rewritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3159b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryInputReWritter(BaseModel):\n",
    "    rewritten_query: str = Field(..., description=\"The query to rewrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ead7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewritter(query: str)->str:\n",
    "    \"\"\"\n",
    "    rewrites the initial query into one that is suitable for web search\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"\"\"\n",
    "         Rewrite the following query to make it more suitable for a web search:\n",
    "         \\nQuery:{query}\n",
    "         \\nRewritten Query:\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QueryInputReWritter)\n",
    "    result = chain.invoke({\"query\": query}).query.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f11ac",
   "metadata": {},
   "source": [
    "#Helper function to pasre web search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d107aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Tuple\n",
    "import json\n",
    "\n",
    "def parse_search_results(result_string: str)-> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a JSON string of search results into a list of title-link tuples.\n",
    "    Args:\n",
    "        results_string (str): A JSON-formatted string containing search results.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples, where each tuple contains the title and link of a search result.\n",
    "                               If parsing fails, an empty list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = json.load(result_string)\n",
    "        return [(result.get(\"title\",\"Untitled\"), result.get(\"url\")) for result in results]\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error parsing search results. Returning empty list\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498d455",
   "metadata": {},
   "source": [
    "#Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "543315a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_documents(query: str, documents: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of documents based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string.\n",
    "        documents (List[str]): A list of document contents to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: A list of relevance scores for each document.\n",
    "    \"\"\"\n",
    "    return [retrieveal_evaluator(query, doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5ca0429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_web_search(query: str) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Perform a web search based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to search for.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], List[Tuple[str, str]]]: \n",
    "            - A list of refined knowledge obtained from the web search.\n",
    "            - A list of tuples containing titles and links of the sources.\n",
    "    \"\"\"\n",
    "    rewritten_query = query_rewritter(query)\n",
    "    web_results = web_search.run(rewritten_query)\n",
    "    web_knowledge = knowledge_refinement(web_results)\n",
    "    sources = parse_search_results(web_results)\n",
    "    return web_knowledge, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed4ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, knowledge: str, sources: List[Tuple[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response to a query using knowledge and sources.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string.\n",
    "        knowledge (str): The refined knowledge to use in the response.\n",
    "        sources (List[Tuple[str, str]]): A list of tuples containing titles and links of the sources.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response_prompt = PromptTemplate(\n",
    "        input_variables=[\"query\", \"knowledge\", \"sources\"],\n",
    "        template=\"Based on the following knowledge, answer the query. Include the sources with their links (if available) at the end of your answer:\\nQuery: {query}\\nKnowledge: {knowledge}\\nSources: {sources}\\nAnswer:\"\n",
    "    )\n",
    "    input_variables = {\n",
    "        \"query\": query,\n",
    "        \"knowledge\": knowledge,\n",
    "        \"sources\": \"\\n\".join([f\"{title}: {link}\" if link else title for title, link in sources])\n",
    "    }\n",
    "    response_chain = response_prompt | llm\n",
    "    return response_chain.invoke(input_variables).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc2ae0",
   "metadata": {},
   "source": [
    "#CRAG Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350cc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crag_process(query: str, faiss_index: FAISS) -> str:\n",
    "    \"\"\"\n",
    "    Process a query by retrieving, evaluating, and using documents or performing a web search to generate a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string to process.\n",
    "        faiss_index (FAISS): The FAISS index used for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response based on the query.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing query: {query}\")\n",
    "\n",
    "    # Retrieve and evaluate documents\n",
    "    retrieved_docs = retrieveal_evaluator(query, faiss_index)\n",
    "    eval_scores = evaluate_documents(query, retrieved_docs)\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(retrieved_docs)} documents\")\n",
    "    print(f\"Evaluation scores: {eval_scores}\")\n",
    "\n",
    "    # Determine action based on evaluation scores\n",
    "    max_score = max(eval_scores)\n",
    "    sources = []\n",
    "    \n",
    "    if max_score > 0.7:\n",
    "        print(\"\\nAction: Correct - Using retrieved document\")\n",
    "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
    "        final_knowledge = best_doc\n",
    "        sources.append((\"Retrieved document\", \"\"))\n",
    "\n",
    "    elif max_score < 0.3:\n",
    "        print(\"\\nAction: Incorrect - Performing web search\")\n",
    "        final_knowledge, sources = perform_web_search(query)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nAction: Ambiguous - Combining retrieved document and web search\")\n",
    "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
    "        # Refine the retrieved knowledge\n",
    "        retrieved_knowledge = knowledge_refinement(best_doc)\n",
    "        web_knowledge, web_sources = perform_web_search(query)\n",
    "        final_knowledge = \"\\n\".join(retrieved_knowledge + web_knowledge)\n",
    "        sources = [(\"Retrieved document\", \"\")] + web_sources\n",
    "\n",
    "    print(\"\\nFinal knowledge:\")\n",
    "    print(final_knowledge)\n",
    "    \n",
    "    print(\"\\nSources:\")\n",
    "    for title, link in sources:\n",
    "        print(f\"{title}: {link}\" if link else title)\n",
    "\n",
    "    # Generate response\n",
    "    print(\"\\nGenerating response...\")\n",
    "    response = generate_response(query, final_knowledge, sources)\n",
    "\n",
    "    print(\"\\nResponse generated\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f077c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Files\\RAG-Techniques\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What are the main causes of climate change?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat are the main causes of climate change?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m vectorstore = Data_Ingestion_Pipe().encode_pdf()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mcrag_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcrag_process\u001b[39m\u001b[34m(query, faiss_index)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Retrieve and evaluate documents\u001b[39;00m\n\u001b[32m     15\u001b[39m retrieved_docs = retrieveal_evaluator(query, faiss_index)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m eval_scores = \u001b[43mevaluate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(retrieved_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_documents\u001b[39m\u001b[34m(query, documents)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_documents\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m]) -> List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Evaluate the relevance of documents based on a query.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m        List[float]: A list of relevance scores for each document.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mretrieveal_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "query = \"What are the main causes of climate change?\"\n",
    "vectorstore = Data_Ingestion_Pipe().encode_pdf()\n",
    "result = crag_process(query, vectorstore)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cd4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
